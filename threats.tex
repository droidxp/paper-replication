\section{Threats}

As any empirical work, this work also has limitations and threats to its validity.

\textbf{Internal Validity.} Due to the randomization behavior presented by each chosen tool, we should not validate the results of this experiment without considering the presence of random events in the execution. To mitigate this, we have used a configuration of the benchmark DroidXP tool that runs multiple times each test and computes the average result from those executions. So, the comparison between the results of this experiment and the experiment presented by Bao et al. could be more precise. Beyond that, we tested only 96 of the original 102 pairs of apps in this experiment because the benchmark could not execute those six pairs of apps due to crashes in the Android emulator. This difference threatens the validity of the comparison between the Bao et al.'s experiment and ours.

\textbf{External Validity.} As this is a replication study, it brings some of the same threats \blls had presented. This study used the same set of apps, which were from a piggy-backed app dataset released by Li et al.~\cite{li2017understanding} So, using this dataset, we could not cover all categories of Android malware. Besides that, we only used a small number of test case generation tools in this study. To mitigate these threats and enrich our research data, we plan to test more apps and use other test case generation tools.

In this study, we used a single tool for Tainted Analysis. With such a small number of tools, we could not have a sample of what a static analysis tool could help in Tainted Analysis. So, we plan to use more tools to mitigate this threat.