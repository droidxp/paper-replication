\section{conclusion and future work}

Mining sandboxes is a popular technique to isolate android applications to analyse their behavior and discover vulnerabilities. Currently most sandbox mining approaches use dynamic analysis and have been studied previously in detail by many including \blls. But the impact of static analysis on these mining approaches is unexplored currently. The use of static analysis techniques, like taint analysis algorithms, to detect malicious behaviours, seems promising and can leverage sandboxes accuracy. However, many authors argue in favor of dynamic analysis for  mining sandboxes, highlight limitations at static analysis at this task, neglecting the complement that could be obtained with it. To the best of our knowledge, this is the first work that explored the impact of static analysis at process of mining Android sandboxes for malware detect.

In this paper, we performed two studies. In the first we conducted a non-exact replication of \blls, considering $96$ pairs apps (B/M) from the original study, and selecting $5$ test generation tools to run for $1$ minute for $2$ times. Firstly, we performed the study, using DroidXP benchmark with its default configuration, enabling the Droidfax static analysis algorithms, and repeat the study by disabling Droidfax, to compute the effective performance of the test generation tools, ignoring the influence of static analysis. For the second experiment, we complement the dynamic analysis using tainted analysis algorithms, through Flowdroid tool, over the same $96$ pairs (B/M) of the previous experiment. The results of the first experiment reveals that, ignoring our fake test case generation tool (Joker), that does not executed the apps, 51.04\%-73.95\% of the malware investigated was detected using the sandboxes with the support of Droidfax static analysis algorithms. Enabling the Droidfax, the sandoxes improved its performances from 23.94\% (Monkey) to 69.39\% (Humanoid). We also find that 19.79\% of malicious apps (19 out of 96) could be detected by none of the sandboxes generated. In the second experiment, the tainted analysis detected 58 among 96 pairs investigated (60.42\%), a better performance than any sandbox constructed by the execution of the test case tools without the Droidfax static analysis algorithms. 

As future work, we plan to investigate other promising tools to enhance our data from previous executions. We will use the benchmark to test tools like Sapienz (a search-based test generation tool)~\cite{DBLP:conf/issta/MaoHJ16} and Dynodroid (an input generation system for Android apps) \cite{DBLP:conf/sigsoft/MachiryTN13}. And we will search for other great tools used by academia and by the industry to be tested by the benchmark DroidXP.
Also, we will execute each of the tools used in this experiment without the benchmark. And, we will compare these results with this experiment's results. This comparison will be valuable in the pursuit of one of this study's goals, which was to analyze what impact the benchmark droidXP has on its results.
