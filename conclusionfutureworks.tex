\section{Conclusions}

Mining sandboxes is a popular technique to isolate android applications to analyse their behavior and discover vulnerabilities. Currently most sandbox mining approaches use dynamic analysis and have been studied previously in detail by many including \blls. But the impact of static analysis on these mining approaches is unexplored currently. The use of static analysis techniques, like taint analysis algorithms, to detect malicious behaviours, seems promising and can leverage sandboxes accuracy. However, many authors argue in favor of dynamic analysis for  mining sandboxes, highlight limitations at static analysis at this task, neglecting the complement that could be obtained with it. To the best of our knowledge, this is the first work that explored the impact of static analysis at process of mining Android sandboxes for malware detect.

In this paper, we performed two studies. In the first we conducted a replication of \blls, considering $96$ pairs apps (B/M) from the original study, and selecting $5$ test generation tools to run for $3$ minute for $3$ times. Firstly, we performed the study, using DroidXP benchmark with its default configuration, enabling the Droidfax static analysis algorithms, and repeat the study by disabling Droidfax, to compute the effective performance of the test generation tools, ignoring the influence of static analysis. For the second experiment, we complement the dynamic analysis using taint analysis algorithms, through Flowdroid tool, over the same $96$ pairs (B/M) of the previous experiment. The results of the first experiment reveals that, ignoring our fake test case generation tool (Joker), that does not executed the apps, $58.33\%$-$76.04\%$ of the malware investigated was detected using the sandboxes with the support of Droidfax static analysis algorithms. The impact of Droidfax in the results range from $16.44\%$ (DroidBot) to $51.79\%$ (Humanoid). We also found that $18.75\%$ of malicious apps (18 out of 96) could be detected by none of the sandboxes generated. In the second experiment, the taint analysis detected 58 among 96 pairs investigated (60.42\%). With the exception of DroidBot tool, taint analysis algorithms had a better performance than any sandbox constructed by the execution of another test case tool, without the Droidfax static analysis algorithms. Our work show that the number of malicious behaviours detected is increased for all cases with taint analysis aid, achieving at the best case, $82.29\%$ (79 among 96) of malware detected with DroidBot.

As future work, we plan to investigate other promising tools to enhance our data from previous executions. We will use the benchmark to test tools like Sapienz (a search-based test generation tool)~\cite{DBLP:conf/issta/MaoHJ16} and Dynodroid (an input generation system for Android apps) \cite{DBLP:conf/sigsoft/MachiryTN13}. And we will search for other great tools used by academia and by the industry to be tested by the benchmark DroidXP.
Also, we will execute each of the tools used in this experiment without the benchmark. And, we will compare these results with this experiment's results. This comparison will be valuable in the pursuit of one of this study's goals, which was to analyze what impact the benchmark DroidXP has on its results.

Our research topic is very critical around the world, and contributions to this domain are necessary and should be considered strategic from the privacy perspective. The present work offered relevant insights into the role of dynamic and static Analysis for Android malware detection, and can be potentially useful for security researchers at Android platform. Our empirical findings demonstrate that static analysis techniques when combined with dynamic analysis at mining sandbox approach is a promising line of work
to improve the task of identifying malicious behavior at Android apps. As a next step, the research community should pursue explore possible static analysis techniques aimed at information flow analysis \cite{DBLP:conf/kbse/ShenVTADLKZ14} in general, and taint analysis \cite{arzt:pldi-2014} specifically, that we believe are possible to be integrated with dynamic analysis techniques, to compose a new mining proposal in sandbox. Another research line should also concentrate on the study of improvement in automatic test generation to modeling sandbox. Some studies point that code coverage of automatic test generation is not high \cite{DBLP:conf/sigsoft/ZengLZXDLYX16}\cite{DBLP:conf/kbse/ChoudharyGO15}, and needs improvement. When we are talking about ``improvement'', we refer not only to the ability to detect bugs but rather coverage of ``typical'' behavior, answering as accurately as possible the question: ``What an Android app actually proposes to do ?''
