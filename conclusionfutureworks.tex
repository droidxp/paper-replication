\section{conclusion and future work}

Static analysis is becoming increasingly important for complement dynamic analysis approaches in the process of mining Android sandboxes. New static analysis techniques, like tainted analysis algorithms can also supports test generator tools for construct sandboxes more  sophisticated. However, state-of-the-art of mining Android sandboxes explore set of sensitive API methods in a Android app, using test generation tools, and highlight limitations at static analysis to mine sandboxes, neglecting the complement that could be obtained with it. To the best of our knowledge, this is the first work that explored the impact of static analysis at process of mining Android sandboxes for malware detect.

In this paper, we performed two studies. In the first we conducted a non-exact replication of Bao et al. work, considering $96$ pairs apps (B/M) from the original study, and selecting $5$ test generation tools to run for $1$ minute for $2$ times. Firstly, we performed the study, using DroidXP benchmark with its default configuration, enabling the Droidfax static analysis algorithms, and a second moment we disables the Droidfax, to compute the effective performance of the test generation tools, ignoring the influence of static analysis. At the second experiment, we complement the dynamic analysis using tainted analysis algorithms, through Flowdroid tool, over the same $96$ pairs (B/M) of the previous experiment. The results of the first experiment reveals that ignoring the Joker tool, 51.04\%-73.95\% of the malware investigated was detected using the sandboxes with the support of Droidfax static analysis algorithms. Enabling the Droidfax staTic analysis, the sandoxes improved its performances from 23.94\% (Monkey) to 69.39\% (Humanoid). We also find that 19.79\% of malicious apps (19 out of 96) could be detected by none of the sandboxes generated. In the second experiment, the tainted analysis detected 58 among 96 pairs investigated (60.42\%), a better performance than any sandbox constructed by the execution of the test case tools without the Droidfax static analysis algorithms.

As future work, we plan to investigate other promising tools to enhance our data from previous executions. We will use the benchmark to test tools like Sapienz (a search-based test generation tool) \cite{DBLP:conf/issta/MaoHJ16} and Dynodroid (an input generation system for Android apps) \cite{DBLP:conf/sigsoft/MachiryTN13}. And we will search for other great tools used by academia and by the industry to be tested by the benchmark DroidXP.
Also, we will execute each of the tools used in this experiment without the benchmark. And, we will compare these results with this experiment's results. This comparison will be valuable in the pursuit of one of this study's goals, which was to analyze what impact the benchmark droidXP has on its results.
