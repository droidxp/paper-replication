\section{Study Settings}

%\kn{This paragraph requires a motivation. The details of Boa's study must be described upfront. Something like "Boa et al.'s study performs ..... Since their study also involved a static analysis component whose impact was not measured, we perform a non-exact replication to understand the impact static analysis may have had in the results...}


In this section
we present the settings of our study, whose goal is to build a general understanding of the implications of using static analysis algorithms
to complement a dynamic analysis approach
for mining Android sandboxes. % We also investigate how static analysis can improve the performance of mining sandboxes, in the task of identifying malicious behavior.
To achieve this goal, we answer the following research questions.

\begin{enumerate}[(RQ1)]
 
 \item What is the impact of the DroidFax static analysis algorithms into the results of the \blls?
  
 \item What is the effective performance of each test generation tool, in terms of the number of detected malware, when we
   discard the contributions of the DroidFax static analysis algorithms?

 \item What are the benefits of using taint
 analysis algorithms to complement the dynamic analysis approach for mining sandboxes?
\end{enumerate}

%\kn{Droidfax comes out of nowhere. We should introduce this or if already introduced earlier, remind the readers of where it stands with the Bao study in the first line of this section when describing boas study}

Answering the research questions RQ1 and RQ2 allows us to expose a possible overestimation in terms of the number of detected malware by each sandbox, constructed by the test
generation tools explored in \blls. Ignoring the implications of DroidFax in the \blls might have introduced a possible threat to their conclusions. Answering research question RQ3
allows us to open up the possibility of finding new strategies for malware detection, complementing the performance of
dynamic analysis through the use of static analysis algorithms.
We conducted two studies to answer the research questions above. In the
first study we leverage DroidXP~\cite{DBLP:conf/scam/CostaMCMVBC20} (Section~\ref{sec:droidxp}) to replicate
the \blls. We present the settings of this first study in Section~\ref{sec:set1}.
In the second study we use
FlowDroid~\cite{DBLP:conf/pldi/ArztRFBBKTOM14} to investigate the 
suitability of taint analysis algorithms to complement mining Android sandboxes. The main idea of this second study is explore the possibility of detected extra malware, do not detected at our first study, through another static analysis strategy based on taint analysis. We present the settings of the second study in Section~\ref{sec:set2}. 


\subsection{The DroidXP benchmark}\label{sec:droidxp}

We used DroidXP to systematically assess and compare the performance of test generation tools for mining android sandboxes. DroidXP relies on a
simple \emph{Command Line Interface} (CLI) that simplifies the integration of different test generation tools and favors the setup and execution 
of the experiments. DroidXP also relies on DroidFax, which instruments Android apps and collects relevant information about
their execution, including the set of sensitive APIs a given
app calls during a test execution. DroidFax also collects inter-component communication (ICC) using  static
program analysis.


The DroidXP CLI provides commands for listing all test case
generation tools (executing the project with the option ``list-tools'') that had been
integrated into the benchmark and commands that execute the experiments. An
experiment run can be configured according to several parameters, including:

\begin{itemize}
    \item \texttt{-tools}: Specifies the test tools used in the experiment
    \item \texttt{-t}: Specifies the threshold (in seconds) for the execution time in the experiment
    \item \texttt{-r}: Specifies the number of repetitions used in the experiment
    \item \texttt{-output-format}: Specifies the output format
    \item \texttt{--debug}: Specifies to run in DEBUG mode (default: false)
    \item \texttt{--disable-static-analysis}: Disable DroidFax static analysis phase (default: false)
\end{itemize}

Figure~\ref{fig:benchArq} shows the DroidXP architecture, based on the pipes-and-filters architectural style \cite{architecture-book}. 
The architecture includes three main components; where each component is responsible for a specific phase of the
benchmark (instrumentation, execution, and result analysis).

\begin{figure*}[thb]
  \includegraphics[width=1\textwidth]{images/benchmark4.png}
  \label{benchArq}
  \caption{Benchmark architecture}
  \label{fig:benchArq}
\end{figure*}
\subsubsection{Phase 1: Instrumentation}

In the first phase, a researcher must define the corpus of APK files DroidXP should consider during a benchmark execution. After that, DroidXP starts the DroidFax service that instruments each APK file, so that DroidXP would be able to collect data about each execution. To improve the performance of the benchmark, the instrumentation phase runs only once for each APK. In this phase, the DroidFax tool also runs some static analysis procedures---when the option \texttt{--disable-static-analysis} is not set.

\subsubsection{Phase 2: Execution}

In this phase, DroidXP installs an (already instrumented) APK file into
an Android emulator, and then executes a test case generation tool
during a period of time. This process repeats for every test case generation
tool and APK files. To provide repeatability of the experiment, DroidXP removes all data stored in the emulator before starting
a new execution. That is, every execution uses a \emph{fresh} emulator,
without any information that might have been kept during
previous executions. 

It is relatively easy to add new test
case generation tools into DroidXP. To achieve this goal, it leverages the Strategy Design
pattern~\cite{patterns-book}, which sets a contract between a family of classes that, in our case, abstracts the particularities
for running each tool we want to integrate into DroidXP.
%To carry out our replication study, we successfully integrated all
%test generation tools previously mentioned into DroidXP: Monkey, DroidBot, DroidMate, and
%Humanoid. 

\subsubsection{Phase 3: Result Analysis}

During the execution of the instrumented apps, all data that is relevant to our
research is collected by Logcat~\cite{Logcat}, one of the Android SDK's native tools. Logcat dumps a log from the Android emulator
while the already instrumented app is in execution. The part of the log we analyze in this phase comprises
the data sent by the methods within the Android app that were instrumented on the first
phase using the DroidFax tool. 

This data includes method coverage from the execution of each test generator tool and the
set of sensitive APIs the app calls during its execution. This set of calls to
sensitive APIs is necessary to estimate the test generator performance in identifying malicious apps---by spotting
differences between the sensitive API accessed by each version of an app (benign or malign).
In the end, the benchmark outputs the results of the experiment, which gives the
performance of one or more testing generator tools in mining sandboxes.

We used the DroidXP infrastructure to conduct the replication of
the \blls, whose settings we present in the following section. 

\subsection{First Study: A replication of the \blls}\label{sec:set1}

The \blls reports the results of an empirical study that compares the performance of test generation tools to mine Android
sandboxes. Since the \blls does not
compute the possible impact of DroidFax into the performance of the test generation tools,
here we replicate their work to understand the impact of the DroidFax static analysis algorithms into the \blls results.

Our replication differs from the original work in a few decisions. First, here we isolate
the effect of the DroidFax static analysis algorithms, in the task to identify malicious apps. In addition, although we use the same dataset of
$102$ pairs of Android apps used in the \blls, here we discarded $6$ pairs for which
we were not able to instrument---out of the $102$ pairs used in the original work, originally shared in the AndroZoo repository~\cite{DBLP:conf/msr/AllixBKT16}. We also introduced a recent test generator tool (Humanoid ~\cite{DBLP:conf/kbse/LiY0C19}), which
has not been considered in the previous work. Finally, we extended the execution time of each test generation tool,
executing each app from the test generation tool for three minutes (instead of one minute in the
original work),
and built the sandboxes after executing each test generation tool
three times---the original work executed each test generation tool
only once. That is, our goal here is not to conduct an
exact replication of the \blls, but instead understand
the role of the DroidFax static analysis algorithms in the
performance of test case generation tools for mining sandboxes.
% The original study executed each app at each tool, for just one minute, and just one time.

Besides Humanoid, our study considers three test generation tools used in the \blls: DroidBot~\cite{DBLP:conf/icse/LiYGC17},
DroidMate~\cite{DBLP:conf/icse/JamrozikZ16}, and Monkey~\cite{Monkey}. We selected DroidBot and DroiMate because they achieved
the best performance on detecting malicious behavior---when considering the $102$ pairs of Android apps (B/M) in the \blls.
It is important to note that here we used a new version of DroidMate (DroidMate-2), since it presents several enhancements
in comparison to the previous version. We also considered the Google's Monkey open source tool, mostly because it is the most
widely used test generation tool for Android~\cite{DBLP:conf/sigsoft/ZengLZXDLYX16}. Monkey is part of the Android SDK
and does not require any additional installation effort. We included Humanoid in our study
because it is a recent tool that emulates realistic users, creating human-like test inputs using deep learning techniques.

\subsubsection{Sensitive APIs extraction process}


In our first study, sensitive APIs extraction process starts with monitoring of invocations of sensitive APIs during the apps (already instrumented) execution by test generation tools under analysis. This task is essential for the process since sensitive APIs are used to access resource sensitives and it call could be a clue of private data leakage. Our study based at a list of 97 sensitive APIs, defined by the AppGuard privacy-control framework~\cite{DBLP:conf/esorics/BackesGHMS13}. Initially, we performances the extraction process using the default settings of DroidXP, (e.g., using DroidFax static analysis component). We explored the four test case generation tools described earlier and a fake test case generation tool (named \joke). 

\joke simulates a test tool that does not execute the Android apps during the exploratory phase. Using this tool the results of the dynamic analysis are not considered, and we can compute the results with only the static analysis component of DroidFax (RQ1). Our study executed each 96 pair of
Android app (B/M) in each test generation tool, including \joke, for three minutes, and for three times.

Then we repeated this process, however this time we used a DroidXP configuration that disables the DroidFax static analysis algorithm.
With this new configuration, we monitored sensitive APIs invocations again, using the same 96 pair of Android app (B/M), the same execution time,
and the same test case generation tools. With this new configuration, became possible computer the effective performance of each test generation tool, in terms of malware detected, discarding the contributions of DroidFax static analysis algorithm (RQ2). 

\subsubsection{Sensitive APIs analysis procedures}

After exploring all sensitive APIs by all test tools, we analyzed the \emph{performance} of these tools to detect malicious behaviors. The procedure starts with the analysis of each sandbox modeled by their respective test generation tools, based on the calls to sensitive APIs.

The test generation tool \emph{performance} is measured as the total number of malware, your respective sandbox is able to identify when comparing the calls to sensitive APIs made by the benign and malign versions of the app. We raise a warning (a \emph{hit}) whenever a sandbox finds that the malicious version of an app calls additional sensitive APIs, in comparison to the calls from the corresponding benign version. Figure \ref{fig:setup} shows all the phases of our experiments, that explore all sensitive resources access by both apps (B/M), and the optional configuration DroidXP, which disables DroidFax static analysis.

Assuming that our fake test case generation tool does not run the Android apps, its \emph{performance} analysis will reveal the DroidFax static algorithms impact at \blls, answering our first research question (RQ1). The analysis of the calls to sensitive APIs made by the 96 pair (B/M), when we disable the DroidFax static analysis component, will reveal the effective \emph{performance} of each test generation test tool under analysis, answering our second research question (RQ2).


\begin{figure}[ht]
  \centering{
   \includegraphics[width=0.75\textwidth]{images/setup3.png}}
   \label{Experiment setup}
   \caption{Experiment setup}
   \label{fig:setup}
 \end{figure}

\subsection{Second Study: Use of Taint Analysis for Malware Identification}\label{sec:set2}

In the second study 
we investigate whether or not a taint-based static analysis approach is also promising for
identifying malwares, given a version of an app that we can assume to be secure.
To this end, we leverage the FlowDroid
taint analysis algorithms for Android apps, at version 2.8, in order to identify dataflows
that might lead to the leakage of sensitive information. Our
goal here is to investigate if it is possible to detect malicious
behavior by means of identifying the \emph{divergent} source-sink paths that FlowDroid reveals after
analysing a benign and a malign versions of an Android app.

\subsubsection{Source-sink paths extraction}

FlowDroid takes as input an Android Application Package (APK file) and
a set of API methods marked either as {\bf source}
or {\bf sink} (or both). Source methods are those that access \emph{sensitive information} (e.g.,
a method that access the address book), while sink methods are those 
that \emph{might share information with external peers} (e.g., a method that
sends messages to a recipient). We rely on the source-sink definitions
of the FlowDroid implementation~\cite{arzt:pldi-2014,rasthofer-source-sink},
which involves a curate list of source and sink methods (including callbacks and
other Android API methods of interest).
FlowDroid then uses a \emph{context, flow, and field
sensitive analysis} to identify all dataflow paths from sources to sinks~\cite{arzt:pldi-2014}.

Source-sink paths extraction involve two steps (see Figure~\ref{fig:settings2}). In the first, we execute FlowDroid to mine the source-sink paths from a benign version of an app, and then enumerate a set (S1) with the 
possible dataflows between sources and sinks. All paths in S1 are considered secure for
our future analysis. In the second step we repeat the FlowDroid execution, though
considering the malicious APK version of the app.
This leads to a second set (S2) of source-sink paths.

\subsubsection{Source-sink paths analysis}

For our analysis it is important to note that not all source-sink paths are malicious, and then we
follow a specific methodology to identify malware in our work: we raise a warning (a \emph{hit}) whenever
FlowDroid finds an additional source-sink path in the malicious version of an app, which
has not been identified when analysing the benign version. For this end, we compute the difference (S3) between the sets S2 and S1 (i.e., $S3 = S2 \setminus S1$), in order to reveal the existence of additional paths in the malicious version (third step). If the set S3 is not empty, we conclude that FlowDroid
identified the malware.

We use two metrics in this second study:
the total number of malicious apps FlowDroid is able to find and the execution time for running the taint analysis algorithm for each app. Moreover, in this second study we use the same dataset of $96$ pairs of Android apps (B/M) used in the First study. Our second study will lead to discover what is the capacity of malware detection of taint analysis algorithms compared to mining sandbox approach, and its potential to complement dynamic analysis for mining sandboxes, thus answering our last research question (RQ3).


\begin{figure}
  \centering{\includegraphics[scale=0.4]{images/second-study-settings.pdf}}
  \caption{Overview of our approach in the second study.}
  \label{fig:settings2}
\end{figure}







