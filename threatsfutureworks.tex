\section{Threats and future works}

Due to the randomization behavior presented by each chosen tool, we should not validate the results of this experiment without considering the presence of random events in the execution. To mitigate this, we have used a configuration of the benchmark DroidXP tool that runs multiple times each test and computes the average result from those executions. So, the comparison between the results of this experiment and the experiment presented by Bao et al. could be more precise.
Beyond that, we tested only 96 of the original 102 pairs of apps in this experiment because the benchmark could not execute those six pairs of apps due to crashes in the Android emulator. This difference threatens the validity of the comparison between the Bao et al.'s experiment and ours.
As future work, we plan to investigate other promising tools to enhance our data from previous executions. We will use the benchmark to test tools like Sapienz (a search-based test generation tool) \cite{DBLP:conf/issta/MaoHJ16} and Dynodroid (an input generation system for Android apps) \cite{DBLP:conf/sigsoft/MachiryTN13}. And we will search for other great tools used by academia and by the industry to be tested by the benchmark DroidXP.
Also, we will execute each of the tools used in this experiment without the benchmark. And, we will compare these results with this experiment's results. This comparison will be valuable in the pursuit of one of this study's goals, which was to analyze what impact the benchmark droidXP has on its results. 
