\section{Introduction}

Almost two-thirds of the world population use mobile technologies~\cite{Comscore}, and the Android Operating System has dominated the market of smartphones, tablets, and others electronic devices. Due to this growing popularity, the number of incidents related to Android malicious software (malware) has significantly increased \cite{DBLP:journals/comsur/FarukiBLGGCR15,DBLP:journals/csur/SufatrioTCT15}. Security issues in Android software applications (``apps'' for short) have become a relevant research topic, and many techniques have been developed to identify vulnerabilities in Android apps \cite{DBLP:conf/pldi/ArztRFBBKTOM14}, including the use of static analysis algorithms to identify source and sink paths involving sensitive data~\cite{DBLP:conf/pldi/ArztRFBBKTOM14} and to reveal the misuse of cryptographic primitives~\cite{krueger:ecoop-2018,rahaman:ccs-2019}. Using a different approach, researchers have also explored the use of dynamic analysis to mine Android sandboxes, whose goal is to protect Android users from malicious behavior~\cite{DBLP:conf/icse/JamrozikSZ16}.

The main idea of mining sandboxes is to explore the set of calls to sensitive API methods, using different approaches for automatically testing Android apps. These tools, explore apps behavior and record all calls to sensitive APIs. A sandbox builds upon the calls to sensitive Android APIs, during the execution of the test cases (exploratory phase). The sandbox could then block future calls to other sensitive resources which diverge from those found in the exploratory phase. Using this approach, the sandbox accuracy depends on the exploratory capabilities of each testing tool used to mine the sandbox rules.
  
Jamrozik et al.~\cite{DBLP:conf/icse/JamrozikSZ16} argue in favor of dynamic analysis for mining sandboxes, instead of using static analysis. The authors claim that dynamic analysis exceeds static analysis on mining Android sandboxes because the analysis must often assume that additional behaviors are possible than actually would be. Otherwise, static analysis might determine that some behavior is impossible, this behavior no longer needs to be checked at runtime. Thereby, code that is interpreted, decrypted, or downloaded at run time is challenging for static analysis---even though these more \emph{dynamic} strategies are often used to introduce malicious behavior. Nonetheless, the authors do not give any empirical validation about the limitations of static analysis to mine sandboxes. In addition, it is not clear how both approaches (dynamic and static analysis) could complement each other in the process of mining Android sandboxes.

The same lack of understanding about the role of static analysis also appears in the work of Bao et al. \cite{DBLP:conf/wcre/BaoLL18}, which present an empirical study that compares different dynamic analysis tools for testing and mining Android sandboxes. Their study leverages DroidFax \cite{DBLP:conf/icsm/CaiR17a} to instrument 102 pairs of Android apps (each pair comprising a benign and a malign version of an App) and to collect the information needed to mine sandboxes (that is, the calls to sensitive APIs).
Although the authors report a performance of at most 70\% of dynamic analysis tools to differentiate the benign and malicious versions of the apps, the authors ignore that DroidFax statically analyzes the Android apps and records calls to sensitive APIs (besides instrumenting the apps), which might lead to an over estimation of the performance of the dynamic analysis tools for mining sandboxes and introduce a possible threat to the conclusions of that work.

Our goal is to understand how static analysis
algorithms might complement dynamic analysis approaches in the process of mining Android sandboxes. To this end, we performed two studies. In the
first, we conduct a non-exact replication of the work of Bao et al.~\cite{DBLP:conf/wcre/BaoLL18}. Our study differs from the original work in two aspects: (a)
by isolating the effect of the static analysis component of DroidFax, (b) by considering more recent dynamic analysis tools for testing Android apps,
and (c) by evaluating the performance of DroidMate~\cite{DBLP:conf/icse/JamrozikZ16} in its original settings. Considering the other dynamic analysis approaches, which are
general purpose test case tools for Android, DroidMate has been designed with the goal of mining Android sandboxes---eventhough it has not been tested adequatly
in the work of Bao et al~\cite{DBLP:conf/wcre/BaoLL18}. In the second study, we explore to what extent we can complement the dynamic analysis approach using
tainted analysis algorithms.


Altogether, this paper brings the following contributions:


\todo[inline]{RB.: Later we can enumerate the paper contributions here.}



%% \begin{enumerate}[(1)]
%%  \item presenting a non-exact replication of the study of the efficiency of test generation tools for mining android sandboxes, using a benchmark solution developed for this specific purpose.
 
%%  \item discovering which are the contribution installments of types of analysis for malware detection task.
 
%%  \item presenting which are malware characteristics, that was not detected by any tool in our analysis dynamic and static experiment.
 
%%  \item presenting data that indicate that static analysis can improve the mining sandboxes process for malware detection.
%%  \end{enumerate}
 
%% We organize the remainder of this paper as follows. Section 2 we present a background and some related works. Section 3 describes all the apps malware used in our experiment, presenting their characterization. Section 4 introduces the study setting of our research. Section 5 presented some results, highlighting some motivation malware example. Section 6 we present the use of a promising static analysis tool, in our motivation example. Finally, Sections 7 and 8, summarizes the paper, discussing our ideas for future works, and presenting the final conclusions.
