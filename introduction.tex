\section{Introduction}\label{sec:introduction}

Almost two-thirds of the world population use mobile technologies~\cite{Comscore}, and the Android Operating System has dominated the market of smartphones, tablets, and others electronic devices \cite{statcounter}. Due to this growing popularity, the number of incidents related to Android malicious software (malware) has significantly increased~\cite{DBLP:journals/comsur/FarukiBLGGCR15,DBLP:journals/csur/SufatrioTCT15}. Security issues in Android software applications~\footnote{In this paper, we will use the terms Android Applications, Android Apps and Apps interchangeably, to represent Android software applications} have become a relevant research topic, and many techniques have been developed to identify vulnerabilities in Android apps~\cite{DBLP:conf/pldi/ArztRFBBKTOM14}, including the use of static analysis algorithms either to identify privacy leaks or to reveal the misuse of cryptographic primitives~\cite{krueger:ecoop-2018,rahaman:ccs-2019}, for instance.

Other approaches rely on the use of dynamic analysis to mine Android sandboxes~\cite{DBLP:conf/icse/JamrozikSZ16}. In this case, during an exploratory phase, one leverages an automatic test case generator tool that runs a benign version of an Android app and collects the set of calls made to sensitive APIs. This set of calls comprises a sandbox infrastructure. The sandbox might then monitor any call to sensitive APIs while the app is running, blocking calls that have not been identified during the exploratory phase---thereby protecting Android users from additional malicious behavior~\cite{DBLP:conf/icse/JamrozikSZ16}.

Jamrozik et al.~\cite{DBLP:conf/icse/JamrozikSZ16} argue in favor of dynamic analysis for mining sandboxes, instead of using static analysis.
The authors state that dynamic analysis exceeds static analysis on mining Android sandboxes because the analysis must often
assume that additional behaviors are possible than actually would be. Many times, codes are interpreted, decrypted, or even downloaded at runtime. In these cases, the static analysis could be ineffective for mining sandboxes, since if static analysis determines that some behavior will not occur, it no longer will be checked at runtime. In addition, code that uses dynamic features (such as reflection) poses additional challenges to static analysis algorithms---even though \emph{dynamic features} of programming languages are often used to introduce malicious behavior. Even though these claims seem reasonable, previous research do not present empirical assessments about the limitations of static analysis to mine sandboxes. Consequently, it is not clear whether and how both approaches (dynamic and static analysis) could complement each other in the process of mining Android sandboxes.

The same lack of understanding about the role of static analysis also appears in the work of Bao et al.~\cite{DBLP:conf/wcre/BaoLL18} (hereafter \blls), which presents an empirical study that compares different dynamic analysis tools for testing and mining Android sandboxes. Their study leverages DroidFax~\cite{DBLP:conf/icsm/CaiR17a} to instrument $102$ pairs of Android apps (each pair comprising a benign and a malign version of an App) (B/M) and to collect the information needed to mine sandboxes (that is, the calls to sensitive APIs).
Although the authors report a precision of at most 70\% of dynamic analysis tools to differentiate the benign and malicious versions of the apps, the authors ignore the fact that DroidFax statically analyzes the Android apps and also records calls to sensitive APIs (besides instrumenting the apps), which might lead to an over estimation of the performance of the dynamic analysis tools for mining sandboxes and introduce a possible threat to the conclusions of that work.

Our goal in this paper is to understand how static analysis
algorithms might complement dynamic analysis approaches in the process of mining Android sandboxes. Here we
present the results of two empirical studies that aim to
address our research goal. In the
first study, we conduct a replication of the \blls, though our research differs from the original work in three aspects: (a)
we isolate the effect of the static analysis component of DroidFax, (b) we included a recent dynamic analysis tool for testing Android apps,
and (c) we extend the execution time and the number of executions of each test case generator tool for all apps
in our dataset. Differently, in the second study we explore how a more advanced static analysis approach
(based on taint analysis) can complement dynamic analysis in the task of mining sandboxes---using
the FlowDroid~\cite{DBLP:conf/pldi/ArztRFBBKTOM14} infrastructure for analysing Android apps.

Along with the research advancements in the area of test case generation, new solutions are constantly emerging, and researchers and practitioners (including ourselves) have come up against technical difficulties in to reproduce the previous studies that compare test case generation tools to mine sandboxes. At our first study, to replicate \blls, we benefit from our benchmark tool, called DroidXP, developed to help researchers and practitioners to integrate test case generation tools and compare
their performance on mining sandboxes at Android platform. The repository for DroidXP is available at GitHub\footnote{https://github.com/droidxp/benchmark} and offers direct access to all source code, installation process, how to run and dependencies. We presented and evaluated DroidXP in a conference paper~\cite{DBLP:conf/scam/CostaMCMVBC20}, and here we discuss the results of a broader investigation that also uses DroidXP to explain the performance of static and dynamic analysis
approaches for mining sandboxes. Altogether, this paper brings the following contributions:

\begin{itemize}
\item A replication of the \blls that better clarifies the performance of
  dynamic analysis tools for mining Android sandboxes.

\item A broad comprehension about the role of static analysis tools for mining
  sandboxes, showing that we can benefit from using both static and dynamic
  analysis for detecting Android malwares.

\item Evidence that a well known static analysis approach, based on
  taint analysis, leads to a performance similar to the dynamic analysis
  approach for mining sandboxes.

\item A replication package of our study that is available on-line, including
  scripts for statistic analysis \footnote{https://htmlpreview.github.io/?https://github.com/droidxp/paper-replication-package/blob/master/replication.html}
  and a benchmark for reproducing and extending our study.
\end{itemize}

%% \todo[inline]{RB: review the following paragraph later}
%% The rest of the paper is organized as follows. Section 2 we present a background and some related works. Section 3 introduces the study setting of our research and Section 4 complements presenting some results and discussion, highlighting motivation malware examples and the use of a promising static analysis tool, on these examples. Finally, Sections 5 and 6, summarizes the paper, presenting some threats, presenting the final conclusions, and discussing our ideas for future works.



%% \begin{enumerate}[(1)]
%%  \item presenting a non-exact replication of the study of the efficiency of test generation tools for mining android sandboxes, using a benchmark solution developed for this specific purpose.
 
%%  \item discovering which are the contribution installments of types of analysis for malware detection task.
 
%%  \item presenting which are malware characteristics, that was not detected by any tool in our analysis dynamic and static experiment.
 
%%  \item presenting data that indicate that static analysis can improve the mining sandboxes process for malware detection.
%%  \end{enumerate}
 
%% We organize the remainder of this paper as follows. Section 2 we present a background and some related works. Section 3 describes all the apps malware used in our experiment, presenting their characterization. Section 4 introduces the study setting of our research. Section 5 presented some results, highlighting some motivation malware example. Section 6 we present the use of a promising static analysis tool, in our motivation example. Finally, Sections 7 and 8, summarizes the paper, discussing our ideas for future works, and presenting the final conclusions.
