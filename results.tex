\section{Results and discussion}

In this section, we present the finding of our study. We first present the results of each study at sub-section (Section V-A and Section V-B), and finish this result session, consolidating our findings and present the main implications of our study.

\subsection{Result of the first study: A non-exact replication}

Our first study performed a non-exact replication of Bao et al work. Our study differs from the original because in your study we isolated the effect of static analysis at our results of malicious apps detected by each mined sandboxes. We could not use all 102 pairs apps (B/M) from the original study, because bechmark tool could not instrument six pair apps from Bao et al work. We also introduced a recent test generator tool, that has not been considered at previous work, Humanoid, and different from the original study, we expand the execution time of each test generation tool, executing each app at each tool for three minutes, while the original study executed for just one minute. The purpose of this time expansion is checking if the test generation tool code coverage results are consistent with Bao et al. results.

Initially, we present the result of 96 pairs apps analyses, performing DroidXP benchmark with its default configuration. Except for the Joke dummy tool, our studies show that 73.95\%-51.04\% of malicious pairs apps investigated, can be detected by the sandboxes constructed by others selected test generation tool. Even though Joke tool does not execute the Android apps during benchmark execution, it was responsible to detect 43.75\% of malwares, only aided by static analysis Droidfax. Table \ref{tab:malware} presents the number of malwares detected by the sandboxes designed for each test case generation tool. The result of the first step of this study, shows that the sandbox constructed by Droidbot, Droidmate, Humanoid, Joke and Monkey identified, 68, 64, 49, 42 and 71 malwares apps among the 96 pairs apps analysed, respectively. Sandboxes build by running Monkey detects more apps with malicious behavior than the other tools analysed.

\begin{table}[ht]
\centering
\begin{tabular}{lcc}\toprule
 Test Generation Tool & Malwares & \\
 & Detected & \% \\ \midrule
 Monkey & 71 & 73.95 \\
 Droidmate & 64 &  66.66  \\
 Humanoid & 49 & 51.04  \\
 Joke & 42 & 43.75  \\
 Droidbot & 68 & 70.83  \\\midrule
 
\end{tabular} 
\caption{Malwares detected in 96 pair (B/M)}
\label{tab:malware}
\end{table}

In the original study, the test case generation tool that detected the largest number of malicious apps was Droidbot, 68 among 102 (66,66\%), the same number of apps find in our experiment, followed by Droimate, that in original study, detected 54 among 102 (52.94\%). This minor variation may be caused due the original work used the oldest version of Droidmate, and our study used Droidmate 2 \cite{DBLP:conf/kbse/BorgesHZ18}, an improved version of the original DroidMate project, that we believe has a better mine strategy than his previous version and may have generated a more efficient sandbox. About Monkey tool, due to his random exploration strategy, his results may vary between studies. Although it had the best performance at our study, at original study it got the third best performance, detecting 48 among 102 pairs investigated (47.05\%). The sandboxes built by running the only test generation tool, not present at Bao et al. work, Humanoid, had the worse performance, identifying a little more than half of the pairs analyzed, 49.

This first setup of our first study also shows that all the constructed sandboxes did not identify the malwares in 19 pairs among 96 (19.79\%). These 19 malicious apps, according to Euphony tool \cite{hurier2017euphony}, that infer a single label per malicious application based on VirusTotal, were: 13 adwares, 3 trojans, 2 PUP (Potentially Unwanted Program) and 1 exploit.

Next, we replicated the study, disabling the Droidfax static analysis algorithm, to investigate the number of malwares detected by the sandbox, generated by the effective performance of the dynamic analysis tools, ignoring the influence of static analysis. Since Joke tool depends exclusively static analysis Droidfax for build it sandbox, we suppose that it sandbox was unable to detect malwares at our data set, result confirmed in our study, where Joke was unable to detect any malware, reducting its sandbox accuracy in 100\%. The sandboxes constructed by the other test generation tool detected 56.25\%-15.62\% of malicious pair apps investigated, reporting an average reduction at the accuracy of each sandbox in 27.78\%. This study setup shows that sandbox conceived by Droidbot, Droidmate, Humanoid, Joke, and Monkey identified: 54, 48, 15, 0 and  54 malwares among 96 pairs apps investigated, respectively, evidencing an average total reduction of 47.78\% of sandboxes efficiency constructed by the test generation tools under analysis. Supporting the results of second step of the first study, Table \ref{tab:malwareWithout} displays the results of the number of malwares detected by the sandboxes, designed by each tool, without static analysis Droidfax aid. 

\begin{table}[ht]
\centering
\begin{tabular}{lccc}\toprule
 Test Generation & Malwares & & Reduction\\
 Tool & Detected & \% & Without s.a. (\%)\\ \midrule
 Monkey & 54 & 56.25 & 23.94\\
 Droidmate & 48 &  50.00 & 25 \\
 Humanoid & 15 & 15.62 & 69.38  \\
 Joke & 0 & 0 & 100 \\
 Droidbot & 54 & 56.25 & 20.58  \\\midrule
 
\end{tabular} 
\caption{Malwares detected in 96 pair (B/M) without static analysis aid}
\label{tab:malwareWithout}
\end{table}

This time, Monkey and Droidbot detected the same and the largest number (54) of malicious apps, and Joke tool, as described, can not detect malwares without static analysis aid. Table \ref{tab:malwareWithout} also show the efficiency loss with the static analysis withdrawal. Humanoid was the most affected by the absence of static analysis, losing 69.38\% of its effectiveness with this benchmark setting. Droidbot was the least affected by this new situation.

When analyzing the number of pairs apps, that none of the sandboxes could not identify malwares, the results are the same of the first step of this study, evidencing that static analysis provided by Droidfax, could not be useful in the task of detecting malware at this specific set of pairs.

Based on these results, we provide evidence that, although static analysis Droidfax, do not affect detection malwares at 19 specific pair apps, its absence impact on sandboxes performance, impairing the accuracy of all of them.

\subsection{Result of the second study: Tainted analysis algorithms.}

To complement our initial survey, we performed ...


%Here we can write about MOTIVATION EXAMPLES.