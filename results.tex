\section{Results and discussion}

In this section we detail the findings of our study. We first present the results of each study
in Section~\ref{sec:res-fs} and Section~\ref{sec:res-ss}. In Section~\ref{sec:discussion} we summarize the
implications of our study. 

\subsection{Result of the first study: A non-exact replication}\label{sec:res-fs}

Our first study performed a non-exact replication of Bao et al work.
Our study differs from the original because here we isolated the effect
of the DroidFax static analysis algorithms in the performance to
identify malicious apps using the test case generation tools for
mining sandboxes. In addition, we discarded six pairs of
Android apps we were not able to instrument---out of the $102$ pairs used in the \original work.
\rb{All of this following information should be detailed in
the study settings, and not repeated here if it is already there.} {\color{red}We also introduced a recent test generator tool, that has not been considered at previous work, Humanoid, and different from the original study, we expand the execution time of each test generation tool, executing each app at each tool for three minutes, while the original study executed for just one minute. The purpose of this time expansion is to check if the test generation tool code coverage results are consistent with Bao et al. results.}

As discussed in the previous section, we first executed the analysis using the DroidXP benchmark with its default configuration. After that, we executed the analysis again though disabling the DroidFax static analysis algorithm, so that we could better estimate the performance of the dynamic analysis tools for mining Android sandboxes. Table~\ref{tab:fs} summarizes the results of th execution. The columns Exec. (I) and Exec. (II) 
show the number of malwares identified when executing each tool (I) with the
support of the DroidFax static analysis algorithms and (II) withouth the support
of DroidFax static analysis algorithms. The Improvement column shows the impact
(in percentage) of DroidFax static analysis algorithms in the results.
In the previous work~\cite{}, the authors do not present a
discussion about the influence of DroidFax in the results, even
though here we report that this difference is in the
range from 23.94\% (Monkey) to 69.39\% (Humanoid)---discarding our
\joke tool for wich DroidFax improves its performance in 100\% (as expected).
Next we discuss the result of each individual test generation tool. 

\begin{table}[ht]
  \caption{Summary of the results of the first study. }
  \centering
  \begin{small}
 \begin{tabular}{lrrr}
   \toprule
   Tool & Exec. (I) & Exec. (II) & Improvement (\%) \\   \midrule
  Monkey &  71 &  54 & 23.94 \\ 
  Droidmate &  64 &  48 & 25.00 \\ 
  Droidbot &  68 &  54 & 20.59 \\ 
  Humanoid &  49 &  15 & 69.39 \\ 
  \joke &  42 &   0 & 100.00 \\ 
 \bottomrule
 \end{tabular}
 \end{small}
 \label{tab:fs}
\end{table}


\subsubsection*{Monkey} sandbox for the first execution (Exec. (I)) detected more apps with malicious behavior than any other tool considered in our study (71 out of the 96 pairs of Android apps). Contrasting, in the original study, Monkey got the third-best performance, detecting 48 malwares within the 102 pairs (47.05\%). This difference might be due to the Monkey randomic strategy for test case generation. \rb{I am not convinced that the other tools do not employ any randomness.} For our second execution (Exec. (II)), there is a reduction of 23.94\% in the Monkey's performance, totaling 54 malware detected---which matches the performance of Droidbot for Exec. (II). \rb{Considering only one run for each configuration, that is Exec. (I) and Exec. (II), we cannot assure that the difference is due to disabling the DroidFax static analysis algorithms.}


\subsubsection*{Droidbot} in the first execution (Exec. (I)) its resulting sandbox detected a total of 68 malware among $96$ pairs analyzed (70.83\%). Interesting, this is the same performance of the original study---although the original study analyzed $102$ pairs of Android apps. Also, in the original study, Droidbot was the test case generation tool whose resulting sandbox detected the largest number of malicious apps. However, in our second execution (Exec. (II)), its performance decreased in 20.58 \%. \rb{Not sure if the following sentence is right.} Despite this reduction, it achieved the largest number of malicious apps detected in the second execution.

\subsubsection*{Droimate} in the first execution led to a sandbox that detected 64 apps with malicious behavior (66.66\%). Contrasting, in the original study Droidmate detected 54 among 102 (52.94\%). A possible explanation for this difference is that here we use the more recent version of Droidmate. In the second execution, without the DroidFax static analysis algorithms, the resulting sandbox's performance drops by 25\%, being able to detect 48 out of the 96 pairs of Android apps. 


\rb{I have read the paper until this point.}

\subsubsection*{Humanoid} at the first step of our study had the worse performance, identifying a little more than half of the pairs analyzed, 49 (51.04\%). It is the only tool not present at Bao et al work, so we could not compare our results with its work. Regarding the second part of the study, without static analysis, Humanoid was the most affected by this new situation, losing 69.38\% of its effectiveness, being able to detect just 15 pair apps malicious.

\subsubsection*{\joke}, as described, is a fake tool, that simulates a test tool that does not execute Android apps during a benchmark execution, considering just static analysis results. Even though Joke tool does not execute the Android apps, it was responsible to detect 43.75\% of malware, only aided by static analysis Droidfax, at the default configuration. Ignoring the influence of static analysis, since Joke tool depends exclusively on static analysis Droidfax, we suppose that it sandbox was unable to detect malware at our data set, result confirmed in our study, where Joke was unable to detect any malware, reducing its sandbox accuracy in 100\%.

Except for the \joke dummy tool, our studies show that 73.95\%-51.04\% of malicious pairs apps investigated, can be detected by the sandboxes constructed by others selected test generation tools, with de DroidXP default configuration. This first setup of our first study also shows that all the constructed sandboxes did not identify the malware in 19 pairs among 96 (19.79\%). These 19 malicious apps, according to Euphony tool \cite{hurier2017euphony}, that infer a single label per malicious application based on VirusTotal, were: 13 adwares, 3 trojans, 2 PUP (Potentially Unwanted Program) and 1 exploit. Table \ref{tab:malware} presents the number of malware detected by the sandboxes designed for each test case generation tool, with DroidXP default configuration, with static analysis Droidfax aid.

\begin{table}[ht]
\centering
\begin{tabular}{lcc}\toprule
 Test Generation Tool & Malwares & \\
 & Detected & \% \\ \midrule
 Monkey & 71 & 73.95 \\
 Droidmate & 64 &  66.66  \\
 Humanoid & 49 & 51.04  \\
 Joke & 42 & 43.75  \\
 Droidbot & 68 & 70.83  \\\midrule
 
\end{tabular} 
\caption{Malwares detected in 96 pair (B/M)}
\label{tab:malware}
\end{table}

Disabling the Droidfax static analysis algorithm, to investigate the number of malware detected by the sandbox, ignoring the influence of static analysis, the sandboxes constructed, except for Joke tool, detected 56.25\%-15.62\% of malicious pair apps investigated, evidencing an average total reduction of 47.78\% of sandboxes efficiency constructed without the static analysis aid. Table \ref{tab:malwareWithout} displays the results of the number of malware detected by the sandboxes, without static analysis Droidfax aid. 

\begin{table}[ht]
\centering
\begin{tabular}{lccc}\toprule
 Test Generation & Malwares & & Reduction (\%)\\
 Tool & Detected  & \% & Without s.a.\\ \midrule
 Monkey & 54 & 56.25 & 23.94\\
 Droidmate & 48 &  50.00 & 25 \\
 Humanoid & 15 & 15.62 & 69.38  \\
 Joke & 0 & 0 & 100 \\
 Droidbot & 54 & 56.25 & 20.58  \\\midrule
 
\end{tabular} 
\caption{Malwares detected in 96 pair (B/M) without static analysis aid}
\label{tab:malwareWithout}
\end{table}


When analyzing the number of pairs apps, which none of the sandboxes could identify malware, the results are the same as the first step of this study, evidencing that static analysis provided by Droidfax, could not be useful in the task of detecting malware at this specific set of pairs.

Based on these results, we provide evidence that, although static analysis Droidfax, do not affect detection malware at 19 specific pair apps, its absence impact on sandboxes performance, impairing the accuracy of all of them.

\subsection{Result of the second study: Tainted analysis algorithms.}\label{sec:res-ss}

To complement our initial survey, we performed our second study focusing just on static analysis, but this time, using an algorithm different from the first study, performed by Droidfax. This time, we used tainted analysis algorithms, using FlowDroid tool, that tracking how private information flows through the apps, and if it is leaked in an unwanted way. The results presents were obtained through the analysis of the same data set of the previous study, that is, on the same 96 pairs apps (B/M).

The first results show that with tainted analysis, we had an analysis processing cost of 32.08 seconds per app pair on average, totaling a processing time of 52 minutes to analyze all the pairs. The processing time depends on the size of the app, and the pair that took the fastest time analyzed in 3 seconds and the longest 437 seconds. Regarding the accuracy of tainted analysis algorithms our study also shows that it was responsible to detect 58 among 96 pair apps (60,42\%), a quantity higher than presented by static analysis algorithms Droidfax, suggested by Joke, our fake test case generation tool, that detected 43.75\% of malware as showed at our first study.

Considering the static analysis provided by Droidfax, and Flowdroid, our results showed that both were able to detect 33 pairs in common (34.37\%), and there were 29 pairs that both were not able to detect (30.20\%). We also realized that just 9 pairs were detected by Droidfax and do not by FlowDroid, while 25 pairs were detected by FlowDroid, and do not by Droidfax, evincing a better accuracy of tainted analysis algorithm. Table \ref{tab:comparison} compile the results of this comparison.

\begin{table}[htb]
\centering
\begin{tabular}{lcc}\toprule

Detected by  & Malwares detected & \%   \\ 
(Droidfax/Flowdroid) & (Among 96 pair) & \\ \midrule
Both tools & 33 & 34.37\%  \\
Just Droidfax & 9 &  \\
Just Flowdroid & 25 &  \\
None & 29 & 30.20\% \\\midrule
 
\multicolumn{1}{r}{Total} &   96 \\ \bottomrule
\end{tabular} 
\caption{Comparison between Droidfax and Flowdroid algorithms}
\label{tab:comparison}
\end{table}


Furthermore, we can highlight that among 19 pair apps that at first study, were not detected by the sandboxes constructed by test generation tool, and static analysis Droidfax miner, 4 were detected by Flowdroid; P35, P38, P56, and P94, therefore exposing better accuracy of the tainted analysis algorithms. Among these 4 pairs detected, 2 were trojans, 1 was the exploit, and 1 was adware.

\subsection{Discussion}\label{sec:discussion}

First we find that static analysis summaries had impact in the Bao et al. study. It was responsible for improving the results of the tools by 47.78\% than its executions alone, answering our first research question (RQ1). Second, Table \ref{tab:malwareWithout} summarizes our findings addressing the second research question (RQ2). We realized that disregarding the static analysis algorithms, and discarding Joke tool, among the tools analyzed in your study, Humanoid had the biggest performance drop, and the least affected was Droidbot, proving be the tool with better effective performance, in terms of the number of detected malware. Finally, we answered our last research question (RQ3) when we leverage sandboxes, complementing the dynamic analysis provided by test generation tool with tainted analysis algorithms. Our experiment highlight that 69.79\%-81.25\% of malware in dataset can be now uncovered by the complement of tainted analysis algorithms, evidencing that 
sandboxes can be further boosted when coupled with new static analysis techniques. We found that the number of identified malicious apps detected is increased for all cases, achieving at the best case, 81.25\% with Monkey, a better performance than all five tools explored at Bao et al., even when they combined all the tools together at theirs work 75.49\%. Table \ref{tab:tanted} display the increase of all tools explored at our research.

\begin{table}[ht]
\centering
\begin{tabular}{lccc}\toprule
 Test Generation & FlowDroid & Total & \%\\
 Tool & Increase  &  & \\ \midrule
 Monkey & 7 & 78 & 81.25\\
 Droidmate & 10 &  74 & 77.08 \\
 Humanoid & 20 & 69 & 71.87  \\
 Joke & 25 & 67 & 69.79 \\
 Droidbot & 9 & 77 & 80.20  \\\midrule
 
\end{tabular} 
\caption{Malwares detected in 96 pair (B/M) increased by Tainted analysis Algorithms}
\label{tab:tanted}
\end{table}

%%here we also have to write about the intersection between droidfax and tainted analysis now, and about what 1 reveled and other dont, and otherwise. 


%Here we can write about MOTIVATION EXAMPLES.
