\section{Results and discussion}

In this section, we present the finding of our study. We first present the results of each study at sub-section (Section V-A and Section V-B), and finish this result session, consolidating our findings and present the main implications of our study.

\subsection{Result of the first study: A non-exact replication}

Our first study performed a non-exact replication of Bao et al work. Our study differs from the original because in your study we isolated the effect of static analysis at our results of malicious apps detected by each mined sandbox. We could use all 102 pairs apps (B/M) from the original study, because the benchmark tool could instrument six pair apps from Bao et al work, i.e., P25, P32, P36, P41, P88, and P93. We also introduced a recent test generator tool, that has not been considered at previous work, Humanoid, and different from the original study, we expand the execution time of each test generation tool, executing each app at each tool for three minutes, while the original study executed for just one minute. The purpose of this time expansion is to check if the test generation tool code coverage results are consistent with Bao et al. results.

We present the results of 96 pairs apps analyses, performing DroidXP benchmark with its default configuration, and then we performing DroidXP disabling the Droidfax static analysis algorithm, to investigate the number of malware detected by the sandbox, generated by the effective performance of the dynamic analysis tools, ignoring static analysis influence. Next, we report the detection capability of each tool in terms of malware detected.

\textbf{Droidbot} with DroidXP default configuration detects a total of 68 malware among 96 pairs analyzed (70.83\%), the same number of apps find at original study, although the original study analyzed 102 pairs, the percent is close. At the original study, Droidbot was the test case generation tool that detected the largest number of malicious apps, 68 among 102 (66,66\%). However, when we disabling the static analysis, Droidbot has an efficiency reduction of 20.58 \%, and was able to detect 54 malware. Despite this reduction, it had the largest number of malicious apps detected in this new situation.

\textbf{Monkey} at the first step, created a sandbox that could detect more apps with malicious behavior than the other tools analyzed, 71 among 96 pairs. Although the original study, it got the third-best performance, detecting 48 among 102 pairs investigated (47.05\%), the result between studies could vary due to its random exploration strategy. When disabling static analysis, its accuracy reduced to 23.94\%, totaling 54 malware detected, matching the detect capacity of Droidbot at this configuration.

\textbf{Droimate} with default configuration was able to detect 64 apps with malicious behaviors (66,66\%), and at the original study, it detected 54 among 102 (52.94\%). This minor variation may be caused due the original work used the oldest version of Droidmate, and our study used Droidmate 2 \cite{DBLP:conf/kbse/BorgesHZ18}, an improved version of the original DroidMate project, that we believe has a better mine strategy than his previous version and may have generated a more efficient sandbox. Disabling static analysis, Droidmate has its accuracy reduced at 25\%, being able to detect 48 among 96 pairs analyzed.

\textbf{Humanoid} at the first step of our study had the worse performance, identifying a little more than half of the pairs analyzed, 49 (51.04\%). It is the only tool not present at Bao et al work, so we could not compare our results with its work. Regarding the second part of the study, without static analysis, Humanoid was the most affected by this new situation, losing 69.38\% of its effectiveness, being able to detect just 15 pair apps malicious.

\textbf{Joke}, as described, is a fake tool, that simulates a test tool that does not execute Android apps during a benchmark execution, considering just static analysis results. Even though Joke tool does not execute the Android apps, it was responsible to detect 43.75\% of malware, only aided by static analysis Droidfax, at the default configuration. Ignoring the influence of static analysis, since Joke tool depends exclusively on static analysis Droidfax, we suppose that it sandbox was unable to detect malware at our data set, result confirmed in our study, where Joke was unable to detect any malware, reducing its sandbox accuracy in 100\%.

Except for the Joke dummy tool, our studies show that 73.95\%-51.04\% of malicious pairs apps investigated, can be detected by the sandboxes constructed by others selected test generation tools, with de DroidXP default configuration. This first setup of our first study also shows that all the constructed sandboxes did not identify the malware in 19 pairs among 96 (19.79\%). These 19 malicious apps, according to Euphony tool \cite{hurier2017euphony}, that infer a single label per malicious application based on VirusTotal, were: 13 adwares, 3 trojans, 2 PUP (Potentially Unwanted Program) and 1 exploit. Table \ref{tab:malware} presents the number of malware detected by the sandboxes designed for each test case generation tool, with DroidXP default configuration, with static analysis Droidfax aid.

\begin{table}[ht]
\centering
\begin{tabular}{lcc}\toprule
 Test Generation Tool & Malwares & \\
 & Detected & \% \\ \midrule
 Monkey & 71 & 73.95 \\
 Droidmate & 64 &  66.66  \\
 Humanoid & 49 & 51.04  \\
 Joke & 42 & 43.75  \\
 Droidbot & 68 & 70.83  \\\midrule
 
\end{tabular} 
\caption{Malwares detected in 96 pair (B/M)}
\label{tab:malware}
\end{table}

Disabling the Droidfax static analysis algorithm, to investigate the number of malware detected by the sandbox, ignoring the influence of static analysis, the sandboxes constructed, except for Joke tool, detected 56.25\%-15.62\% of malicious pair apps investigated, evidencing an average total reduction of 47.78\% of sandboxes efficiency constructed without the static analysis aid. Table \ref{tab:malwareWithout} displays the results of the number of malware detected by the sandboxes, without static analysis Droidfax aid. 

\begin{table}[ht]
\centering
\begin{tabular}{lccc}\toprule
 Test Generation & Malwares & & Reduction (\%)\\
 Tool & Detected & \% & Without s.a.\\ \midrule
 Monkey & 54 & 56.25 & 23.94\\
 Droidmate & 48 &  50.00 & 25 \\
 Humanoid & 15 & 15.62 & 69.38  \\
 Joke & 0 & 0 & 100 \\
 Droidbot & 54 & 56.25 & 20.58  \\\midrule
 
\end{tabular} 
\caption{Malwares detected in 96 pair (B/M) without static analysis aid}
\label{tab:malwareWithout}
\end{table}


When analyzing the number of pairs apps, which none of the sandboxes could identify malware, the results are the same as the first step of this study, evidencing that static analysis provided by Droidfax, could not be useful in the task of detecting malware at this specific set of pairs.

Based on these results, we provide evidence that, although static analysis Droidfax, do not affect detection malware at 19 specific pair apps, its absence impact on sandboxes performance, impairing the accuracy of all of them.

\subsection{Result of the second study: Tainted analysis algorithms.}

To complement our initial survey, we performed our second study focusing just on static analysis, but this time, using an algorithm different from the first study, performed by Droidfax. This time, we used tainted analysis algorithms, using FlowDroid tool, that tracking how private information flows through the apps, and if it is leaked in an unwanted way. The results presents were obtained through the analysis of the same data set of the previous study, that is, on the same 96 pairs apps (B/M).

The first results show that with tainted analysis, we had an analysis processing cost of 32.08 seconds per app pair on average, totaling a processing time of 52 minutes to analyze all the pairs. The processing time depends on the size of the app, and the pair that took the fastest time analyzed in 3 seconds and the longest 437 seconds. Regarding the accuracy of tainted analysis algorithms our study also shows that it was responsible to detect 58 among 96 pair apps (60,42\%), a quantity higher than presented by static analysis algorithms Droidfax, suggested by Joke, our fake test case generation tool, that detected 43.75\% of malware as showed at our first study.

Considering the static analysis provided by Droidfax, and Flowdroid, our results showed that both were able to detect 33 pairs in common (34.37\%), and there were 29 pairs that both were not able to detect (30.20\%). We also realized that just 9 pairs were detected by Droidfax and do not by FlowDroid, while 25 pairs were detected by FlowDroid, and do not by Droidfax, evincing a better accuracy of tainted analysis algorithm. Table \ref{tab:recursos} compile the results of this comparison.

\begin{table}[htb]
\centering
\begin{tabular}{lcc}\toprule

Detected by  & Malwares detected & \%   \\ 
(Droidfax/Flowdroid) & (Among 96 pair) & \\ \midrule
Both tools & 33 & 34.37\%  \\
Just Droidfax & 9 &  \\
Just Flowdroid & 25 &  \\
None & 29 & 30.20\% \\\midrule
 
\multicolumn{1}{r}{Total} &   96 \\ \bottomrule
\end{tabular} 
\caption{Comparison between Droidfax and Flowdroid algorithms}
\label{tab:recursos}
\end{table}


Furthermore, we can highlight that among 19 pair apps that at our first study, were not detected by the sandboxes constructed by test generation tool and static analysis Droidfax miner, 4 could be detected by Flowdroid; P35, P38, P56, and P94, therefore exposing, better accuracy of the static analysis algorithms. Among these 4 pairs detected, 2 were trojans, 1 was the exploit, the only no detected, and 1 was adware.



%%here we also have to write about the intersection between droidfax and tainted analysis now, and about what 1 reveled and other dont, and otherwise. 


%Here we can write about MOTIVATION EXAMPLES.